
def accessTextCorpora(fileid, word):
    file_words = inaugural.words(fileid)
    wordcoverage = int(len(file_words)/len(set(file_words)))
    ed_words = [words for words in set(file_words) if words.endswith('ed')]
    textfreq2 = [word.lower() for word in file_words if word.isalpha()]
    textfreq = nltk.FreqDist(textfreq2)
    wordfreq = textfreq[word]
    
    return wordcoverage, ed_words, wordfreq

def createUserTextCorpora(filecontent1, filecontent2):
    # Write your code here
    corpusdir = 'nltk_data/'
    with open(corpusdir + 'content1.txt', 'w') as text_file:
        text_file.write(filecontent1)
    with open(corpusdir + 'content2.txt', 'w') as text_file:
        text_file.write(filecontent2)

    text_corpus = PlaintextCorpusReader(corpusdir, 
    ["content1.txt", "content2.txt"])

    no_of_words_corpus1 = len(text_corpus.words("content1.txt"))
    no_of_unique_words_corpus1 = len(set(text_corpus.words("content1.txt")))

    no_of_words_corpus2 = len(text_corpus.words("content2.txt"))
    no_of_unique_words_corpus2 = len(set(text_corpus.words("content2.txt")))
    
    return text_corpus, no_of_words_corpus1, no_of_unique_words_corpus1, no_of_words_corpus2, no_of_unique_words_corpus2 


def calculateCFD(cfdconditions, cfdevents):
    stopword = set(stopwords.words('english'))
    cdev_cfd = nltk.ConditionalFreqDist([(genre, word.lower()) for genre in  cfdconditions for word in brown.words(categories=genre) if not word.lower()  in stopword])
    inged_cfd = [ (genre, word.lower()) for genre in brown.categories() for word  in brown.words(categories=genre) if (word.lower().endswith('ing') or word.lower().endswith('ed')) ]
    inged_cfd = [list(x) for x in inged_cfd]
    cdev_cfd.tabulate(conditions = cfdconditions, samples = cfdevents)

    for wd in inged_cfd:
        if wd[1].endswith('ing') and wd[1] not in stopword:
            wd[1] = 'ing'
        elif wd[1].endswith('ed') and wd[1] not in stopword:
            wd[1] = 'ed'
#print(inged_cfd)
    inged_cfd = nltk.ConditionalFreqDist(inged_cfd)
#print(inged_cfd.conditions())    
    inged_cfd.tabulate(conditions=cfdconditions, samples = ['ed','ing'])


def processRawText(textURL):
    # Write your code here
    content = request.urlopen(textURL).read()
    textcontent = content.decode('unicode_escape')
    pattern = r'\w+'
    tokenizedlcwords = nltk.word_tokenize(textcontent)
    
    noofwords = len(tokenizedlcwords)
    noofunqwords = len(set([word.lower() for word in tokenizedlcwords]))
    
    
    wordcov = int(noofwords/noofunqwords)
    
    wordfreq = nltk.FreqDist([word.lower() for word in tokenizedlcwords if word.isalpha()])
    
    maxfreq = wordfreq.max()
    
    return noofwords, noofunqwords, wordcov, maxfreq



import nltk

from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer

#
# Complete the 'performBigramsAndCollocations' function below.
#
# The function accepts following parameters:
#  1. STRING textcontent
#  2. STRING word
#

def performBigramsAndCollocations(textcontent, word):
    # Write your code here
    tokenizedword = nltk.tokenize.regexp_tokenize(textcontent, pattern = '\w*', gaps = False)
    #Step 2
    tokenizedwords = [x.lower() for x in tokenizedword if x != '']

    tokenizedwordsbigram=list(nltk.bigrams(tokenizedwords))
    stop_words = set(stopwords.words('english')) 
    filteredwords = []
    for x in tokenizedwordsbigram:
       if x not in stop_words:
          filteredwords.append(x)
     
    tokenizednonstopwordsbigram = nltk.ConditionalFreqDist(filteredwords)  
    print(tokenizednonstopwordsbigram[word].most_common(3))
    mostfrequentwordafter = tokenizednonstopwordsbigram[word].most_common(3)
    gen_text=nltk.Text(tokenizedwords).collocation_list()
    
    collocationwords = [i[0]+" "+i[1] for i in gen_text]
    
    return mostfrequentwordafter, collocationwords


from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk import PorterStemmer
from nltk import LancasterStemmer
from nltk import WordNetLemmatizer
#
# Complete the 'performStemAndLemma' function below.
#
# The function accepts STRING textcontent as parameter.
#

def performStemAndLemma(textcontent):
    # Write your code here
    # Write your code here
    tokenizedword = nltk.tokenize.regexp_tokenize(textcontent, pattern = '\w*', gaps = False)
    # Step 2
    tokenizedwords = [y for y in tokenizedword if y != '']
    unique_tokenizedwords = set(tokenizedwords)
    tokenizedwords = [x.lower() for x in unique_tokenizedwords if x != '']
    
    stop_words = set(stopwords.words('english')) 
    filteredwords = []
    for x in tokenizedwords:
       if x not in stop_words:
          filteredwords.append(x)
          
    porter = PorterStemmer()
    porterstemmedwords = [porter.stem(word) for word in (filteredwords) ]
    
    lancaster = LancasterStemmer()
    lancasterstemmedwords = [lancaster.stem(word) for word in (filteredwords) ]
    
    wnl = WordNetLemmatizer()
    lemmatizedwords = [wnl.lemmatize(word) for word in (filteredwords) ]
    
    return porterstemmedwords, lancasterstemmedwords, lemmatizedwords
    

def tagPOS(textcontent, taggedtextcontent, defined_tags):
    # Write your code here
    words = nltk.word_tokenize(textcontent)
    nltk_pos_tag = nltk.pos_tag(words)
    
    tagged_pos_tag =  [ nltk.tag.str2tuple(word) for word in taggedtextcontent.split() ]
    
    baseline_tagger = nltk.UnigramTagger(model=defined_tags)
    unigram_pos_tag = baseline_tagger.tag(words)
    
    return nltk_pos_tag, tagged_pos_tag, unigram_pos_tag
    

