{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "# Training additional entity types using spaCy\n",
    "from __future__ import unicode_literals, print_function\n",
    "import pickle\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New entity labels\n",
    "# Specify the new entity labels which you want to add here\n",
    "LABEL = ['I-geo', 'B-geo', 'I-art', 'B-art', 'B-tim', 'B-nat', 'B-eve', 'O', 'I-per', 'I-tim', 'I-nat', 'I-eve', 'B-per', 'I-org', 'B-gpe', 'B-org', 'I-gpe']\n",
    "\n",
    "\"\"\"\n",
    "geo = Geographical Entity\n",
    "org = Organization\n",
    "per = Person\n",
    "gpe = Geopolitical Entity\n",
    "tim = Time indicator\n",
    "art = Artifact\n",
    "eve = Event\n",
    "nat = Natural Phenomenon\n",
    "\"\"\"\n",
    "\n",
    "def main(n_iter=10, TRAIN_DATA):\n",
    "    \"\"\"Setting up the pipeline and entity recognizer, and training the new entity.\"\"\"\n",
    "    nlp = spacy.load(\"en_core_web\")  # load existing spacy model\n",
    "    print(\"Created blank 'en' model\")\n",
    "    ner = nlp.get_pipe('ner')\n",
    "    for i in LABEL:\n",
    "        ner.add_label(i)   # Add new entity labels to entity recognizer\n",
    "    optimizer = nlp.begin_training()\n",
    "    # Get names of other pipes to disable them during training to train only NER\n",
    "\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "    # Test the trained model\n",
    "    test_text = 'Gianni Infantino is the president of FIFA.'\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "\n",
    "    nlp.meta['name'] = \"new_model\"  # rename model\n",
    "    nlp.to_disk(\"NLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the saved model\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "doc2 = nlp2(test_text)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House - ORG - Companies, agencies, institutions, etc.\n",
      "three - CARDINAL - Numerals that do not fall under another type\n",
      "2 - CARDINAL - Numerals that do not fall under another type\n"
     ]
    }
   ],
   "source": [
    "# Perform standard imports import spacy nlp = spacy.load('en_core_web_sm')\n",
    "# Write a function to display basic entity info: \n",
    "def show_ents(doc): \n",
    "    if doc.ents: \n",
    "        for ent in doc.ents: \n",
    "            print(ent.text+' - '+ent.label_+ ' - '+str(spacy.explain(ent.label_))) \n",
    "    \n",
    "doc1 = nlp(\"House has three bedrooms and 2 bathrooms\") \n",
    "show_ents(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "#Import the displaCy library\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pipeline component\n",
    "ner=nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding labels to the `ner`\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable pipeline components you dont need to change\n",
    "\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(30):\n",
    "\n",
    "    # shuffling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('BMW', 'PRODUCT')]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "\n",
    "doc = nlp(\"I was driving a BMW\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
